coding-agent/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ agent_loop.py
â”‚   â”‚   â”œâ”€â”€ planner.py
â”‚   â”‚   â”œâ”€â”€ memory.py
â”‚   â”‚   â””â”€â”€ tool_manager.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ shell_tool.py
â”‚   â”‚   â”œâ”€â”€ python_tool.py
â”‚   â”‚   â”œâ”€â”€ file_tool.py
â”‚   â”‚   â”œâ”€â”€ git_tool.py
â”‚   â”‚   â”œâ”€â”€ installer_tool.py
â”‚   â”‚   â”œâ”€â”€ web_search_tool.py
â”‚   â”‚   â””â”€â”€ os_tool.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ openai_llm.py
â”‚   â”‚   â”œâ”€â”€ groq_llm.py
â”‚   â”‚   â”œâ”€â”€ local_llm.py
â”‚   â”‚   â””â”€â”€ model_selector.py
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ rg_search.py
â”‚   â”‚   â”œâ”€â”€ tags_client.py
â”‚   â”‚   â”œâ”€â”€ indexer.py
â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”œâ”€â”€ templates.py
â”‚   â”‚   â””â”€â”€ rag_query.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ json_parser.py
â”‚       â”œâ”€â”€ file_ref.py      â† handles @file references
â”‚       â””â”€â”€ os_detect.py
â”‚
â”œâ”€â”€ cli/
|   â”œâ”€â”€ main.py
|   â””â”€â”€ __init__.py
|
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ tools.yaml
â”‚   â””â”€â”€ models.yaml
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ agent.log
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_tools.py
â”‚   â”œâ”€â”€ test_agent.py
â”‚   â””â”€â”€ test_llm.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ setup.py OR pyproject.toml


ğŸ“˜ Coding Agent â€“ README

A lightweight, extensible terminal-based coding agent that uses planning, memory, tools, and multiple LLM providers (OpenAI, Groq, Ollama, or local models) to execute tasks.

This project provides:

ğŸ”„ Agent loop (Planner â†’ Tools â†’ LLM â†’ Memory)

ğŸ§  Memory system (auto switch between Redis or in-memory backend)

ğŸ› ï¸ Tool Manager (auto-load tools from agent/tools/)

ğŸ¤– Multi-LLM support (OpenAI / Groq / Ollama / local)

ğŸ–¥ï¸ CLI interface (Typer-based, easy to run)

ğŸ“¦ Installation
1. Clone the repository
git clone <your-repo-url>
cd coding-agent

2. Install dependencies
pip install -r requirements.txt


Or if using poetry:

poetry install

3. (Optional) Install Redis for memory backend

If Redis is running at redis://localhost:6379/0, the agent automatically uses Redis-based memory.

Otherwise, it falls back to in-memory storage.

âš™ï¸ Environment Configuration

The following environment variables control LLM providers:

Provider	Variable	Example
OpenAI API	OPENAI_API_KEY	export OPENAI_API_KEY="sk-..."
Groq API	GROQ_API_KEY	export GROQ_API_KEY="gsk_..."
Ollama local models	No key needed	ensure ollama is installed
Redis memory	REDIS_URL	export REDIS_URL="redis://localhost:6379/0"

Example:

export OPENAI_API_KEY="sk-xxxx"
export GROQ_API_KEY="gsk-xxxx"
export REDIS_URL="redis://localhost:6379/0"


You may also configure defaults in:

configs/config.yaml
configs/models.yaml
configs/tools.yaml

ğŸš€ Running the CLI

The main CLI entrypoint lives in:

cli/main.py


Run it directly:

python -m cli.main run "Write a python script that prints prime numbers."


Or if installed via pip install -e ., run:

coding-agent run "Generate a hello world program."

ğŸ§° CLI Commands
1ï¸âƒ£ Run a single prompt
coding-agent run --provider openai --model gpt-4o "Build a flask server with 2 endpoints"


Options:

--provider, -p        (openai | groq | ollama | local)
--model, -m           Model name
--persist             Enable memory persistence (local JSON)
--persist-path        File path for memory storage
--max-memory          Max memory entries
--verbose/--no-verbose


Example:

coding-agent run -p groq -m mixtral-8x7b "Optimize this SQL query"

2ï¸âƒ£ Interactive REPL mode

Start a persistent session with memory, tools, and streaming:

coding-agent repl -p ollama -m llama3


Inside REPL, you can talk to your agent continuously.

3ï¸âƒ£ List all available tools
coding-agent list-tools


Example output:

[
  {"name": "shell", "module": "agent.tools.shell_tool", "class": "ShellTool"},
  {"name": "python", "module": "agent.tools.python_tool", "class": "PythonTool"},
  {"name": "file", "module": "agent.tools.file_tool", "class": "FileTool"}
]

4ï¸âƒ£ Probe available LLM providers
coding-agent probe


Or probe a specific provider:

coding-agent probe -p groq


Example output:

{
  "openai": {"available": true, "msg": "API key OK"},
  "groq":   {"available": true, "msg": "API key OK"},
  "ollama": {"available": true, "msg": "Running locally"}
}

5ï¸âƒ£ Inspect or clear memory

Show stored memory items:

coding-agent memory --show


Clear memory:

coding-agent memory --clear


Limit output:

coding-agent memory --show --limit 10

6ï¸âƒ£ Show or select provider/model
coding-agent select


Select from preference order:

coding-agent select -p "groq,openai,ollama"

ğŸ§  Memory Backend
Automatic selection:
Condition	Memory Used
Redis installed & reachable	RedisMemory
Else	InMemory (local)

No config changes are needed â€” it's automatic.

To force Redis:

export REDIS_URL="redis://localhost:6379/0"

ğŸ§± Project Structure
coding-agent/
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ core/ (agent loop, planner, memory, tool manager)
â”‚   â”œâ”€â”€ tools/ (shell, python, git, file, installer, web search)
â”‚   â”œâ”€â”€ llm/ (openai, ollama, groq, local)
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ cli/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ configs/
â”œâ”€â”€ logs/
â”œâ”€â”€ tests/
â””â”€â”€ README.md

ğŸ› ï¸ Development Setup
Install in editable mode:
pip install -e .

Run tests:
pytest -q

ğŸ“Œ Example Usage
Ask agent to generate and apply code changes:
coding-agent run "Add a new CLI option --dry-run to my Python tool"

Use shell + git tools to modify your repo:
coding-agent run "Create feature branch and update README"

Use local LLM via Ollama:
coding-agent run -p ollama -m codellama "Refactor these functions for readability."

ğŸ‰ You're all set!

Your coding agent is now ready to run with:

coding-agent repl

::coding-agent <- name depends on the object file you create after compiling c code
